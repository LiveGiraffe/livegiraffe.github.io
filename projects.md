---
layout: page
title: Projects
permalink: /projects/
navigation_weight: 1
description: "Academic and personal projects"
---

## Draw Music

Inspired by Iannis Xenakis' UPIC, Draw Music is a software program that allows the user to draw on a rectangular canvas and play it back. Sound is created from the drawing with left to right representing time and top to bottom representing low to high frequency. 

The functionality is quite simple but the fine grained control means the only limit to the acoustic control is the resolution of the drawing canvas. With practice and an increased toolset, like any highly-skilled instrument, it should be able to produce very interesting and nuanced music.

---

## Onaip

![Onaip picture showing push buttons](/assets/onaipA.jpg)
![Onaip picture showing linear position sensors](/assets/onaipC.jpg)
![Onaip picture showing wiring](/assets/onaipB.jpg)

My goal in this project was to experiment with different sensor types, musical instrument design, synthesis mappings and Arduino programming.

There are three play surfaces on the Onaip and whichever is facing upwards is active:

- three force-sensitive resistors (FSRs) for percussive playing
- eight push buttons to be paired with a breath sensor for potential as a woodwind-like controller
- three linear positions sensors for chords and glissando

---

## RAMS: Radial audio menu system

![RAMS diagram showing the user's head in the center of repeating sounds, with one louder than the other.](/assets/rams_topdown.png)

This project examined the potential for an eyes-free mobile phone menu by using simple gestural input and spatialized audio. An eyes-free menu is useful for visually impaired users or for those whose vision is otherwise occupied. The goal was to discover the usability issues of such a menu. I created it with the cool kids Farid Rener, Jacob Beard, and Rajkuman Viswanathan and with help from [Institut Nazareth et Louis-Braille](http://www.inlb.qc.ca/).

The items of the menu were presented as up to 8 sounds around the users head. As the user moved his finger around the selection wheel, the items would come into focus, meaning they were played louder and more clearly.

#### Usability Issues

We discovered and examined several usability issues with such an interface, for example:

**Audio Overload**

Even with the [cocktail party effect](http://en.wikipedia.org/wiki/Cocktail_party_effect), there is a limit to how much audio a person can take in at once. We attempted to alleviate audio overload by:

- a 'fish-eye' gain level for the items with the loudest at the focussed items 
- low-pass filtering non-focussed items so that their presence could be heard but wouldn't be as distracting
- staggering the starts of neighbouring sounds 

**Sound Design**
After testing several sound designs [(earcons, auditory icons, spearcons)](http://sonify.psych.gatech.edu/publications/pdfs/2008ICAD-DinglerLindsayWalker.pdf), we decided to short, simple names generated by text-to-speech because of the clarity of the meaning to new users as well as their ability to be created without arbitrary decisions by humans.

As well, an optional auditory cursor played white noise that followed the user's finger around the selection wheel in order to reinforce the spatialization illusion and the link between finger position and places in space.

**Input Gestures**
The menu was designed to be used eyes-free and so was tested with blind users. As a result of that testing, the control and button layout was made as simple as possible. Also, a 'press and hold' paradigm with auditory confirmations was implemented for the accept and back buttons to reduce the number of accidental selections and confusion. 

Lastly, we had a 'hover-over' paradigm similar to mouse-over on GUIs which gave more information (such as phone number when a contact name was hovered over). This was a potentially time-saving feature but caused confusion with users. In an actual implementation, a consistent logic to distinguish between hover/select button would need to be developed.

---

## Open Orchestra's performance analysis and visualization

![Open Orchestra's four-screen interface](/assets/oo_monolito.png)

The Open Orchestra is the musical equivalent of an aircraft simulator, providing student musicians a high-fidelity ensemble performance experience. As part of that project, this research developed methods of computer analysis and visualization for feedback and comparison. To identify measures of interest, the software compares the pitch and timing of a student's performance with that of the original musician playing the same part.

A study on the efficacy of visualizations as feedback was presented at the Visualization and Data Analysis conference in January 2012.

---

## Trumpet Tone Quality

![A recording setup with two microphones and a music stand.](/assets/trumpet.jpg)

Broadly, this research sought to answer the question, "Can computers give feedback on a musician's tone quality?" This work was first presented at [ISMIR 2011](https://trevorknight.squarespace.com/s/Knight-PotentialForAutomaticAssessmentOfTrumpetToneQuality.pdf) then formed the basis of [my master's thesis](http://digitool.library.mcgill.ca/R/?func=dbin-jump-full&object_id=110681).

The abstract from my thesis: 

> This work examines which audio features, the components of recorded sound, are most relevant to trumpet tone quality by using classification and feature selection. A total of 10 trumpet players with a variety of experience levels were recorded playing the same notes under the same conditions. Twelve musical instrumentalists listened to the notes and provided subjective ratings of the tone quality on a seven-point Likert scale to provide training data for classification. The initial experiment verified that there is statistical agreement between human raters on tone quality and that it was possible to train a support vector machine (SVM) classifier to identify different levels of tone quality with success of 72% classification accuracy with the notes split into two classes and 46% when using seven classes. In the main experiment, different types of feature selection algorithms were applied to the 164 possible audio features to select high-performing subsets. The baseline set of all 164 audio features obtained a classification accuracy of 58.9% with seven classes tested with cross-validation. Ranking, sequential floating forward selection, and genetic search produced accuracies of 43.8%, 53.6%, and 59.6% with 20, 21, and 74 features, respectively. Future work in this field could focus on more nuanced interpretations of tone quality or on the applicability to other instruments.